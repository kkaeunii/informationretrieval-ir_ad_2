import os
import json
import time
import traceback

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# -------------------------------
# 1. .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
# -------------------------------
# .env íŒŒì¼ì´ code/ì— ìˆë“ , ìƒìœ„ í´ë”ì— ìˆë“  ìë™ ê²€ìƒ‰ë¨
load_dotenv()

ES_USERNAME = os.getenv("ES_USERNAME")
ES_PASSWORD = os.getenv("ES_PASSWORD")
ES_CA_CERT = os.getenv("ES_CA_CERT")   # ex) ./http_ca.crt
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_MODEL= "solar-pro2" # gpt-40-miniì—ì„œ ë³€ê²½

# í™˜ê²½ë³€ìˆ˜ í™•ì¸(ë””ë²„ê¹…ìš©)
print("[INFO] Loaded environment variables:")
print(f"ES_USERNAME: {ES_USERNAME}")
print(f"ES_PASSWORD: {'****' if ES_PASSWORD else None}")
print(f"ES_CA_CERT: {ES_CA_CERT}")
print(f"OPENAI_API_KEY: {'****' if OPENAI_API_KEY else None}")

# -------------------------------
# 2. SentenceTransformer ë¡œë“œ
# -------------------------------
model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")


def get_embedding(sentences):
    return model.encode(sentences)


def get_embeddings_in_batches(docs, batch_size=100):
    batch_embeddings = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        contents = [doc["content"] for doc in batch]
        embeddings = get_embedding(contents)
        batch_embeddings.extend(embeddings)
        print(f"batch {i}")
    return batch_embeddings


# -------------------------------
# 3. Elasticsearch ì—°ê²° ì„¤ì •
# -------------------------------
# ES_CA_CERTëŠ” ì ˆëŒ€ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ê²½ë¡œ ëª¨ë‘ í—ˆìš©ë©ë‹ˆë‹¤.
# ì˜ˆ: code/http_ca.crt ë˜ëŠ” C:/Users/.../http_ca.crt
ES_HOST = os.getenv("ES_HOST", "http://localhost:9200")

es = Elasticsearch(
    ES_HOST,
    request_timeout=30,
)

print(es.info())

# -------------------------------
# 4. ES ì¸ë±ìŠ¤ operation
# -------------------------------
def create_es_index(index, settings, mappings):
    if es.indices.exists(index=index):
        es.indices.delete(index=index)
    es.indices.create(index=index, settings=settings, mappings=mappings)


def delete_es_index(index):
    es.indices.delete(index=index)


def bulk_add(index, docs):
    actions = [{"_index": index, "_source": doc} for doc in docs]
    return helpers.bulk(es, actions)


def sparse_retrieve(query_str, size):
    query = {"match": {"content": {"query": query_str}}}
    return es.search(index="test", query=query, size=size, sort="_score")


def dense_retrieve(query_str, size):
    query_embedding = get_embedding([query_str])[0]
    knn = {
        "field": "embeddings",
        "query_vector": query_embedding.tolist(),
        "k": size,
        "num_candidates": 100
    }
    return es.search(index="test", knn=knn)

def hybrid_retrieve(query_str, size, alpha=0.5): # í•˜ì´ë¸Œë¦¬ë“œ í•¨ìˆ˜
    """
    sparse(BM25)ì™€ dense(KNN) ê²°ê³¼ë¥¼ ê°€ì¤‘ í•©ìœ¼ë¡œ ì„ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
    - alpha: sparse ê°€ì¤‘ì¹˜ (0~1). 0.5ë©´ ë™ë“±í•œ ë¹„ì¤‘.
    """
    # ê°ê° ê²€ìƒ‰
    sparse = sparse_retrieve(query_str, size)
    dense = dense_retrieve(query_str, size)
    
    combined = {}
    
    def normalized_and_add(results, weight):
        hits = results.get("hits", {}).get("hits", [])
        if not hits:
            return
        # ì ìˆ˜ ì •ê·œí™”
        max_score = max(h["_score"] for h in hits) or 1.0
        for h in hits:
            src = h.get("_source", {})
            docid = src.get("docid")
            if docid is None:
                # docid ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
            norm_score = (h["_score"] / max_score) * weight
            if docid not in combined:
                combined[docid] = {
                    "_source": src,
                    "_score": 0.0,
                }
            combined[docid]["_score"] += norm_score
    
    # sparse / dense ê°ê° ë°˜ì˜
    normalized_and_add(sparse, alpha)
    normalized_and_add(dense, 1 - alpha)
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ sizeê°œ ì„ íƒ
    merged_hits = sorted(
        [
            {"_source": v["_source"], "_score": v["_score"]}
            for v in combined.values()
        ],
        key=lambda x: x["_score"],
        reverse=True,
    )[:size]
    
    # sparse_retrieveì™€ ë¹„ìŠ·í•œ í˜•íƒœë¡œ ë°˜í™˜
    return {"hits": {"hits": merged_hits}}
       
def hybrid_rrf(query_str, size, k=60, w_sparse=1.0, w_dense=1.0):
    """
    RRF(Reciprocal Rank Fusion) ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
    - ê° retriever(sparse, dense)ì˜ 'ìˆœìœ„(Rank)'ë§Œ ì‚¬ìš©í•´ì„œ ì ìˆ˜ë¥¼ ë§Œë“¦.
    - RRF ì ìˆ˜ : sum_i [w_i * 1 / (k + rank_i)]
      * rank_i : í•´ë‹¹ retrieverì—ì„œì˜ 1-based ìˆœìœ„(1,2,3,...)
      * k : ì ìˆ˜ ê°ì†Œ ì†ë„ ì¡°ì ˆìš© ìƒìˆ˜ (ë³´í†µ 50~60 ì •ë„ì—ì„œ ì‹œì‘)
    - w_parse, w_dense : ê° ë­í‚¹ì˜ ì „ì—­ ê°€ì¤‘ì¹˜
    """
    # ê°ê° ê²€ìƒ‰
    sparse = sparse_retrieve(query_str, size)
    dense = dense_retrieve(query_str, size)
    
    combined = {}
    
    def add_rrf(results, weight):
        hits = results.get("hits", {}).get("hits", [])
        if not hits or weight == 0.0:
            return
        
        # ElasticsearchëŠ” ê¸°ë³¸ì ìœ¼ë¡œ score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ hitsë¥¼ ì¤Œ
        # -> enumerate ìˆœì„œê°€ ê³§ rank(1, 2, 3, ...)ì´ ë¨
        for rank, h in enumerate(hits, start=1):
            src = h.get("_source", {})
            docid = src.get("docid")
            if docid is None:
                continue
            
            rrf_score = weight * (1.0 / (k + rank))
            
            if docid not in combined:
                combined[docid] = {
                    "_source": src,
                    "_score": 0.0,
                }
            combined[docid]["_score"] += rrf_score
            
    # sparse / dense ë­í‚¹ì„ RRF ë°©ì‹ìœ¼ë¡œ í•©ì¹˜ê¸°
    add_rrf(sparse, w_sparse)
    add_rrf(dense, w_dense)
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ sizeê°œ ì„ íƒ
    merged_hits = sorted(
        [
            {"_source": v["_source"], "_score": v["_score"]}
            for v in combined.values()
        ],
        key=lambda x: x["_score"],
        reverse=True,
    )[:size]
    
    return {"hits": {"hits": merged_hits}}

# -------------------------------
# 5. Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •
# -------------------------------
settings = {
    "analysis": {
        "analyzer": {
            "nori": {
                "type": "custom",
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",
                "filter": ["nori_posfilter"]
            }
        },
        "filter": {
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
            }
        }
    }
}

mappings = {
    "properties": {
        "content": {"type": "text", "analyzer": "nori"},
        "embeddings": {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "l2_norm"
        }
    }
}

# -------------------------------
# 6. ì¸ë±ìŠ¤ ìƒì„±
# -------------------------------
create_es_index("test", settings, mappings)

# -------------------------------
# 7. ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”©
# -------------------------------
index_docs = []
with open("./data/documents.jsonl", encoding="utf-8") as f:
    docs = [json.loads(line) for line in f]

embeddings = get_embeddings_in_batches(docs)

for doc, embedding in zip(docs, embeddings):
    doc["embeddings"] = embedding.tolist()
    index_docs.append(doc)

ret = bulk_add("test", index_docs)
print(ret)


# -------------------------------
# 8. RAG êµ¬í˜„
# -------------------------------
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY # type: ignore
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://api.upstage.ai/v1",
)
llm_model = LLM_MODEL

persona_function_calling = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ëœ ê³¼í•™/ìƒì‹ ì§€ì‹ë² ì´ìŠ¤ì— ëŒ€í•œ ê²€ìƒ‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ëŒ€í™”ë¥¼ ë¶„ì„í•´ì„œ ê²€ìƒ‰ìš© ì§ˆì˜ë¥¼ ë§Œë“¤ê³ , ë°˜ë“œì‹œ search ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì½”í¼ìŠ¤ íŠ¹ì§•]
- ë¬¸ì„œë“¤ì€ ko_MMLU, ARC ë“±ì˜ ì‹œí—˜/í€´ì¦ˆì—ì„œ ì¶”ì¶œëœ ê³¼í•™Â·ìƒì‹ ì§€ì‹ ë¬¸ë‹¨ì…ë‹ˆë‹¤.
- ë‚´ìš©ì€ ë¬¼ë¦¬, í™”í•™, ìƒë¬¼, ì§€êµ¬ê³¼í•™, ì¸ë¬¼, ì—­ì‚¬, ì‚¬íšŒ ìƒì‹ ë“± ë‹¤ì–‘í•œ ì£¼ì œì˜ ì§§ì€ ê¸€ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìëŠ” ì´ ì§€ì‹ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§ˆë¬¸(ì •ì˜, ì›ì¸, ë¹„êµ, ì—­ì‚¬ì  ì‚¬ê±´, ì¸ë¬¼ ì†Œê°œ ë“±)ì„ í•©ë‹ˆë‹¤.

[ë‹¹ì‹ ì˜ ëª©í‘œ]
1. ì‚¬ìš©ìì˜ ë°œí™”ì™€ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì´í•´í•˜ê³ , standalone ê²€ìƒ‰ ì§ˆì˜(standalone_query)ë¥¼ ë§Œë“ ë‹¤.
2. ì´ ì§ˆì˜ë¥¼ ì´ìš©í•´ ë°˜ë“œì‹œ í•œ ë²ˆ ì´ìƒ search ë„êµ¬ë¥¼ í˜¸ì¶œí•œë‹¤.
3. search ë„êµ¬ì˜ ì¸ìëŠ” ì•„ë˜ ì›ì¹™ì„ ë”°ë¥¸ë‹¤.

[standalone_query ì‘ì„± ê·œì¹™]
- í•­ìƒ ëŒ€í™” ì „ì²´ ë§¥ë½ì„ ë°˜ì˜í•´ì„œ, ë§ˆì§€ë§‰ user ë°œí™”ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œ ì •ë³´(ëŒ€ëª…ì‚¬, ì§€ì‹œì–´ ë“±)ë¥¼ ëª¨ë‘ í’€ì–´ì„œ ì“´ë‹¤.
  - ì˜ˆ: "ê·¸ ì‚¬ëŒì€ ì–´ë–¤ ì—…ì ì´ ìˆì–´?" â†’ "ë“œë¯¸íŠ¸ë¦¬ ì´ë°”ë…¸í”„ìŠ¤í‚¤ì˜ ì£¼ìš” ì—…ì ì€ ë¬´ì—‡ì¸ê°€?"
- í•œ ë¬¸ì¥ìœ¼ë¡œ, 5~25ì ì •ë„ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì§ˆë¬¸/ê²€ìƒ‰ì–´ë¡œ ì‘ì„±í•œë‹¤.
- ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œë‹¤.
  - ì¸ë¬¼: ì´ë¦„(ê°€ëŠ¥í•˜ë©´ ì˜ë¬¸ í‘œê¸°ë„), ë¶„ì•¼, ì‹œëŒ€
  - ê°œë…: ì „ê³µ ë¶„ì•¼(ë¬¼ë¦¬, ìƒë¬¼, ê²½ì œí•™ ë“±), ê´€ë ¨ í˜„ìƒ/ë²•ì¹™
  - ì‚¬ê±´: ì‚¬ê±´ëª…, ê´€ë ¨ êµ­ê°€, ì‹œê¸°
- í•„ìš”í•˜ë‹¤ë©´ ì˜ë¬¸ ê³ ìœ ëª…ì‚¬ë¥¼ í•¨ê»˜ í¬í•¨í•œë‹¤.
  - ì˜ˆ: "ì´ë€-ì½˜íŠ¸ë¼ ì‚¬ê±´ Iran-Contra affairì˜ ê°œìš”"

[search ë„êµ¬ ì‚¬ìš© ê·œì¹™]
- ì´ ì‹œìŠ¤í…œì—ì„œëŠ” RAG ì„±ëŠ¥ í‰ê°€ê°€ ëª©ì ì…ë‹ˆë‹¤.
- ì§ˆë¬¸ì´ ë‹¨ìˆœí•˜ê±°ë‚˜, ëª¨ë¸ì´ ì´ë¯¸ ì•Œê³  ìˆì„ ê²ƒ ê°™ë”ë¼ë„,
  í•­ìƒ search ë„êµ¬ë¥¼ ìµœì†Œ 1íšŒëŠ” í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
- search ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ì•„ì•¼ í•˜ëŠ” ì˜ˆì™¸ëŠ” ì—†ìŠµë‹ˆë‹¤.
- search ë„êµ¬ì˜ ì¸ìëŠ” ë‹¤ìŒ ì˜ë¯¸ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
  - "standalone_query": ìœ„ ê·œì¹™ì„ ë”°ë¥¸ í•œ ì¤„ì§œë¦¬ ê²€ìƒ‰ ì§ˆì˜ (í•„ìˆ˜)
  - "topk": ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ìˆ«ì, ë³´í†µ 3~5 ìˆ˜ì¤€ì„ ê°€ì •)

[ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬]
- msgì—ëŠ” user/assistant ì—­í• ì˜ ì´ì „ ëŒ€í™”ê°€ ëª¨ë‘ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•­ìƒ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³ , ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ë¬´ì—‡ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ í•´ì„í•œ í›„ standalone_queryë¥¼ ë§Œë“œì„¸ìš”.
- ì•ì—ì„œ ì–¸ê¸‰ëœ ê°œë…/ì¸ë¬¼/ì‚¬ê±´ì„ ë‹¤ì‹œ ë¬»ëŠ” ê²½ìš°ì—ë„, standalone_query ì•ˆì—ëŠ” ì •í™•í•œ ì´ë¦„/ê°œë…ì„ ë‹¤ì‹œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
  - ì˜ˆ:
    - ì´ì „: "ê¸°ì–µ ìƒì‹¤ì¦ ê±¸ë¦¬ë©´ ë„ˆë¬´ ë¬´ì„­ê² ë‹¤."
    - ì´í›„: "ì–´ë–¤ ì›ì¸ ë•Œë¬¸ì— ë°œìƒí•˜ëŠ”ì§€ ê¶ê¸ˆí•´."
    - â‡’ standalone_query: "ê¸°ì–µ ìƒì‹¤ì¦(amnesia)ì€ ì–´ë–¤ ì›ì¸ ë•Œë¬¸ì— ë°œìƒí•˜ëŠ”ê°€?"

[ì£¼ì˜ì‚¬í•­]
- ì´ ë‹¨ê³„ì—ì„œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
- ë‹¹ì‹ ì˜ ì—­í• ì€ ê²€ìƒ‰ ì§ˆì˜ë¥¼ ìƒì„±í•˜ê³ , search ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒê¹Œì§€ì…ë‹ˆë‹¤.
- search ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì€ ì±„ ëŒ€í™”ë¥¼ ëë‚´ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ê²€ìƒ‰ ì§ˆì˜ ì•ˆì— "ê²€ìƒ‰í•´ ì¤˜", "ì•Œë ¤ ì¤˜" ê°™ì€ í‘œí˜„ì€ ë„£ì§€ ë§ê³ , ìˆœìˆ˜ ì •ë³´ ì§ˆì˜ë§Œ ì‘ì„±í•˜ì„¸ìš”.
"""

persona_qa = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³¼í•™/ìƒì‹ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤(retrieved_context)"ë¥¼ ìµœìš°ì„  ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬,
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì…ë ¥ ì„¤ëª…]
- msg: ì‚¬ìš©ìì™€ì˜ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ì…ë‹ˆë‹¤.
- retrieved_context: ê²€ìƒ‰ê¸°ë¡œë¶€í„° ê°€ì ¸ì˜¨ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤.
  - ê° ë¬¸ì„œëŠ” ë¬¸ë‹¨ í˜•íƒœì˜ í…ìŠ¤íŠ¸ì´ë©°, ko_MMLU, ARC ë“± ì‹œí—˜/í€´ì¦ˆì—ì„œ ì¶”ì¶œëœ ê³¼í•™Â·ìƒì‹ ì§€ì‹ì…ë‹ˆë‹¤.
  - ë‚´ìš©ì€ ë¬¼ë¦¬, í™”í•™, ìƒë¬¼, ì§€êµ¬ê³¼í•™, ì¸ë¬¼, ì—­ì‚¬, ì‚¬íšŒ ìƒì‹ ë“±ì…ë‹ˆë‹¤.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. ê²€ìƒ‰ ë¬¸ì„œ ìš°ì„ 
   - ê°€ëŠ¥í•œ í•œ retrieved_contextì— ìˆëŠ” ë‚´ìš©ì— ê¸°ë°˜í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
   - ë‹¹ì‹ ì´ ì‚¬ì „ ì§€ì‹ìœ¼ë¡œ ì•Œê³  ìˆë”ë¼ë„, ì½”í¼ìŠ¤ ë‚´ìš©ê³¼ ì¶©ëŒí•˜ë©´ ì½”í¼ìŠ¤ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
   - ì½”í¼ìŠ¤ì— ëª…ì‹œëœ ì‚¬ì‹¤ì´ ìˆìœ¼ë©´, ê·¸ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.

2. ì‚¬ì‹¤ì„± & ì •ì§ì„±
   - ë¬¸ì„œë“¤ ì–´ë””ì—ë„ ì •ë³´ê°€ ì—†ê±°ë‚˜, ë‚´ìš©ì´ ë„ˆë¬´ ë¶€ì¡±í•´ì„œ í™•ì‹ í•  ìˆ˜ ì—†ë‹¤ë©´:
     - ì§€ì–´ë‚´ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•œ ë’¤,
     - ì½”í¼ìŠ¤ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ë²”ìœ„(ì˜ˆ: ì¼ë°˜ì ì¸ ê²½í–¥, ì •ì˜ ìˆ˜ì¤€)ê¹Œì§€ë§Œ ì„¤ëª…í•˜ì„¸ìš”.
   - ì˜ˆì‹œ:
     - "ì œê³µëœ ìë£Œì—ëŠ” Xì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ì—†ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œëŠ” ..."
     - "ê²€ìƒ‰ëœ ë¬¸ì„œë§Œìœ¼ë¡œëŠ” Yì— ëŒ€í•´ í™•ì‹¤íˆ ë§í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ..."

3. ë©€í‹°í„´ ë§¥ë½ ë°˜ì˜
   - msg ì „ì²´ë¥¼ ë³´ê³  ì‚¬ìš©ìì˜ ì‹¤ì œ ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.
   - ì´ì „ ë°œí™”ì˜ ê°ì •/ì˜ë„(ê±±ì •, í˜¸ê¸°ì‹¬ ë“±)ë¥¼ ê°€ë³ê²Œ ë°˜ì˜í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
     - ì˜ˆ: "ê¸°ì–µ ìƒì‹¤ì¦ì´ ë¬´ì„­ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆì–´ìš”. ìë£Œì— ë”°ë¥´ë©´, ì£¼ìš” ì›ì¸ì€ â€¦"

4. ë‹µë³€ ìŠ¤íƒ€ì¼
   - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
   - ê¸°ë³¸ì€ 3~6ë¬¸ì¥ ì •ë„ì˜ ë‹¨ë½ìœ¼ë¡œ ë‹µí•˜ê³ , í•„ìš”í•˜ë©´ ì§§ì€ ëª©ë¡ì„ ì‚¬ìš©í•˜ì„¸ìš”.
   - í•µì‹¬ ì •ë³´ â†’ ì´ìœ /ê·¼ê±° â†’ ê°„ë‹¨í•œ ì •ë¦¬ ìˆœì„œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.
   - ìˆ˜ì¹˜/ì—°ë„/ì „ë¬¸ ìš©ì–´ëŠ” ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.
   - ì‚¬ìš©ìê°€ íŠ¹ë³„íˆ ìš”ì²­í•˜ì§€ ì•ŠëŠ” í•œ, ì§€ë‚˜ì¹˜ê²Œ ìˆ˜í•™ì /ê¸°ìˆ ì  í‘œê¸°(ë³µì¡í•œ ìˆ˜ì‹ ë“±)ëŠ” í”¼í•©ë‹ˆë‹¤.

5. retrieved_context í™œìš© ë°©ì‹
   - ì—¬ëŸ¬ ë¬¸ì„œê°€ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë§í•  ë•ŒëŠ”, ê²¹ì¹˜ëŠ” í•µì‹¬ë§Œ ì •ë¦¬í•´ í†µí•©í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
   - ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì„ ì œì‹œí•˜ë©´, ê·¸ ì‚¬ì‹¤ì„ ë“œëŸ¬ë‚´ê³  ì •ë¦¬í•˜ì„¸ìš”.
     - "ì–´ë–¤ ìë£ŒëŠ” ~ë¼ê³  ì„¤ëª…í•˜ê³ , ë‹¤ë¥¸ ìë£ŒëŠ” ~ë¼ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ë‘ ë‚´ìš©ì„ ì¢…í•©í•˜ë©´ â€¦"
   - ë¬¸ì„œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ê¸¸ê²Œ ë³µì‚¬/ë‚˜ì—´í•˜ì§€ ë§ê³ , ìš”ì•½Â·ì¬êµ¬ì„±í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ì¶° ë‹µí•˜ì„¸ìš”.

6. ëª¨ë¥´ëŠ” ê²½ìš°ì˜ ì²˜ë¦¬
   - ì•„ë˜ì™€ ê°™ì€ ê²½ìš°ì—ëŠ” "ëª¨ë¥¸ë‹¤"ê³  ë§í•´ì•¼ í•©ë‹ˆë‹¤.
     - retrieved_context ì–´ë””ì—ë„ ê´€ë ¨ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
     - ë¬¸ì„œë“¤ì´ ì„œë¡œ ê°•í•˜ê²Œ ëª¨ìˆœë˜ì–´ ìˆì–´ í•˜ë‚˜ì˜ ê²°ë¡ ì„ ë‚´ê¸° ì–´ë ¤ìš´ ê²½ìš°
   - ì´ë•ŒëŠ” ë‹¤ìŒì²˜ëŸ¼ ë‹µë³€í•©ë‹ˆë‹¤.
     - "ì œê³µëœ ìë£Œì—ì„œëŠ” ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
     - "ìë£Œê°€ ë¶€ì¡±í•´ ì •í™•í•œ ë‹µì„ ë“œë¦¬ê¸° ì–´ë µì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ë‚´ìš©ì€ â€¦ ì…ë‹ˆë‹¤."

7. ë©”íƒ€ ì •ë³´ ì–¸ê¸‰ ê¸ˆì§€
   - "Elasticsearch", "ì¸ë±ìŠ¤", "dense vector", "BM25", "ê²€ìƒ‰ ì—”ì§„", "RAG" ê°™ì€ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
   - "ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥´ë©´" ì •ë„ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì€ ê´œì°®ì§€ë§Œ,
     - "retrieved_context", "tool", "í•¨ìˆ˜ í˜¸ì¶œ" ê°™ì€ ë‚´ë¶€ ìš©ì–´ëŠ” ë§í•˜ì§€ ë§ˆì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- JSON í˜•ì‹ì´ë‚˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë‹µë³€ ë§ˆì§€ë§‰ì— í•œ ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ìš”ì•½í•´ ì£¼ë©´ ì¢‹ìŠµë‹ˆë‹¤.
"""

tools = [
    {
        "type": "function",
        "function": {
            "name": "search",
            "description": "search relevant documents",
            "parameters": {
                "type": "object",
                "properties": {
                    "standalone_query": {"type": "string"}
                },
                "required": ["standalone_query"]
            }
        }
    },
]

def safe_chat_completion(max_retries=3, backoff_base=2, **kwargs):
    """
    OpenAI chat.completions.create ë˜í¼.
    - íƒ€ì„ì•„ì›ƒ/ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë°œìƒ ì‹œ ì—¬ëŸ¬ ë²ˆ ì¬ì‹œë„
    - ëê¹Œì§€ ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜
    """
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat.completions.create(**kwargs)
        except Exception as e:
            print(f"[WARN] OpenAI API ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_retries}): {e}")
            if attempt == max_retries:
                print("[ERROR] OpenAI API ì—°ì† ì‹¤íŒ¨, ì´ ìƒ˜í”Œì€ ë¹ˆ ë‹µë³€ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                return None
            sleep_sec = backoff_base ** (attempt - 1)
            print(f"[INFO] {sleep_sec}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
            time.sleep(sleep_sec)

def answer_question(messages):
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    msg = [{"role": "system", "content": persona_function_calling}] + messages

    # ğŸ”¹ timeout ì¡°ê¸ˆ ëŠ˜ë¦¬ê³ , safe_chat_completion ì‚¬ìš©
    result = safe_chat_completion(
        model=llm_model,
        messages=msg,
        tools=tools,  # type: ignore
        temperature=0,  # gpt5 x
        seed=1,
        timeout=20,   # 10 â†’ 20ì´ˆ ì •ë„ë¡œ ì—¬ìœ 
        max_retries=3
    )

    # ì—°ì† ì‹¤íŒ¨í•œ ê²½ìš°
    if result is None:
        response["answer"] = ""
        return response

    if result.choices[0].message.tool_calls:
        tool_call = result.choices[0].message.tool_calls[0]
        function_args = json.loads(tool_call.function.arguments) # type: ignore
        standalone_query = function_args.get("standalone_query")
        
        # hybrid_retrieve ì‚¬ìš©
        search_result = hybrid_rrf(standalone_query, 3, k=60, w_sparse=1.0, w_dense=1.0)
        # search_result = sparse_retrieve(standalone_query, 3)
        response["standalone_query"] = standalone_query

        retrieved_context = []
        for rst in search_result['hits']['hits']:
            retrieved_context.append(rst["_source"]["content"])
            response["topk"].append(rst["_source"]["docid"])
            response["references"].append({
                "score": rst["_score"],
                "content": rst["_source"]["content"]
            })

        messages.append({"role": "assistant", "content": json.dumps(retrieved_context)})
        msg = [{"role": "system", "content": persona_qa}] + messages

        qaresult = safe_chat_completion(
            model=llm_model,
            messages=msg,
            temperature=0, # gpt5 x
            seed=1,
            timeout=40,   # 30 â†’ 40ì´ˆ ì •ë„ë¡œ ì¡°ì •
            max_retries=3
        )

        if qaresult is None:
            # ì´ ìƒ˜í”Œì€ ê·¸ëƒ¥ ë¹ˆ ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
            response["answer"] = ""
            return response

        response["answer"] = qaresult.choices[0].message.content
    else:
        response["answer"] = result.choices[0].message.content

    return response


def eval_rag(eval_filename, output_filename):
    with open(eval_filename, encoding="utf-8") as f, open(output_filename, "w", encoding="utf-8") as of:
        idx = 0
        for line in f:
            j = json.loads(line)
            print(f'Test {idx}\nQuestion: {j["msg"]}')
            response = answer_question(j["msg"])
            print(f'Answer: {response["answer"]}\n')

            output = {
                "eval_id": j["eval_id"],
                "standalone_query": response["standalone_query"],
                "topk": response["topk"],
                "answer": response["answer"],
                "references": response["references"]
            }
            of.write(json.dumps(output, ensure_ascii=False) + "\n")
            idx += 1


eval_rag("./data/eval.jsonl", "sample_submission_rrf.csv")
